{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d512958-629d-43e5-a873-90fced50aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.metrics import Precision, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa6a556-3955-47d0-9b3f-5568bf9635d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Nutrient Data\n",
    "csv_path = 'indian_dishes_nutrients.csv'  # Ensure columns: 'Food', 'Calories', 'Protein', 'Fat', 'Carbs'\n",
    "data = pd.read_csv(csv_path)\n",
    "X_nutrients = data.drop(columns=['Food'])\n",
    "y = data['Food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86583cc-6ec2-4c03-b088-e134d7fb7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Food Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5acfea1e-491d-4c91-9916-937471b89a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 80 classes.\n",
      "Found 800 images belonging to 80 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load Image Data\n",
    "data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "train_images = data_gen.flow_from_directory(\n",
    "    'Indian Food Images',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images = data_gen.flow_from_directory(\n",
    "    'Indian Food Images',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d29fe6-158b-4bdf-9d2a-1ae29c30bcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adhirasam': 0,\n",
       " 'aloo_gobi': 1,\n",
       " 'aloo_matar': 2,\n",
       " 'aloo_methi': 3,\n",
       " 'aloo_shimla_mirch': 4,\n",
       " 'aloo_tikki': 5,\n",
       " 'anarsa': 6,\n",
       " 'ariselu': 7,\n",
       " 'bandar_laddu': 8,\n",
       " 'basundi': 9,\n",
       " 'bhatura': 10,\n",
       " 'bhindi_masala': 11,\n",
       " 'biryani': 12,\n",
       " 'boondi': 13,\n",
       " 'butter_chicken': 14,\n",
       " 'chak_hao_kheer': 15,\n",
       " 'cham_cham': 16,\n",
       " 'chana_masala': 17,\n",
       " 'chapati': 18,\n",
       " 'chhena_kheeri': 19,\n",
       " 'chicken_razala': 20,\n",
       " 'chicken_tikka': 21,\n",
       " 'chicken_tikka_masala': 22,\n",
       " 'chikki': 23,\n",
       " 'daal_baati_churma': 24,\n",
       " 'daal_puri': 25,\n",
       " 'dal_makhani': 26,\n",
       " 'dal_tadka': 27,\n",
       " 'dharwad_pedha': 28,\n",
       " 'doodhpak': 29,\n",
       " 'double_ka_meetha': 30,\n",
       " 'dum_aloo': 31,\n",
       " 'gajar_ka_halwa': 32,\n",
       " 'gavvalu': 33,\n",
       " 'ghevar': 34,\n",
       " 'gulab_jamun': 35,\n",
       " 'imarti': 36,\n",
       " 'jalebi': 37,\n",
       " 'kachori': 38,\n",
       " 'kadai_paneer': 39,\n",
       " 'kadhi_pakoda': 40,\n",
       " 'kajjikaya': 41,\n",
       " 'kakinada_khaja': 42,\n",
       " 'kalakand': 43,\n",
       " 'karela_bharta': 44,\n",
       " 'kofta': 45,\n",
       " 'kuzhi_paniyaram': 46,\n",
       " 'lassi': 47,\n",
       " 'ledikeni': 48,\n",
       " 'litti_chokha': 49,\n",
       " 'lyangcha': 50,\n",
       " 'maach_jhol': 51,\n",
       " 'makki_di_roti_sarson_da_saag': 52,\n",
       " 'malapua': 53,\n",
       " 'misi_roti': 54,\n",
       " 'misti_doi': 55,\n",
       " 'modak': 56,\n",
       " 'mysore_pak': 57,\n",
       " 'naan': 58,\n",
       " 'navrattan_korma': 59,\n",
       " 'palak_paneer': 60,\n",
       " 'paneer_butter_masala': 61,\n",
       " 'phirni': 62,\n",
       " 'pithe': 63,\n",
       " 'poha': 64,\n",
       " 'poornalu': 65,\n",
       " 'pootharekulu': 66,\n",
       " 'qubani_ka_meetha': 67,\n",
       " 'rabri': 68,\n",
       " 'ras_malai': 69,\n",
       " 'rasgulla': 70,\n",
       " 'sandesh': 71,\n",
       " 'shankarpali': 72,\n",
       " 'sheer_korma': 73,\n",
       " 'sheera': 74,\n",
       " 'shrikhand': 75,\n",
       " 'sohan_halwa': 76,\n",
       " 'sohan_papdi': 77,\n",
       " 'sutar_feni': 78,\n",
       " 'unni_appam': 79}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb4828c-a854-43d2-b7ed-ec85f402d471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adhirasam',\n",
       " 'aloo_gobi',\n",
       " 'aloo_matar',\n",
       " 'aloo_methi',\n",
       " 'aloo_shimla_mirch',\n",
       " 'aloo_tikki',\n",
       " 'anarsa',\n",
       " 'ariselu',\n",
       " 'bandar_laddu',\n",
       " 'basundi',\n",
       " 'bhatura',\n",
       " 'bhindi_masala',\n",
       " 'biryani',\n",
       " 'boondi',\n",
       " 'butter_chicken',\n",
       " 'chak_hao_kheer',\n",
       " 'cham_cham',\n",
       " 'chana_masala',\n",
       " 'chapati',\n",
       " 'chhena_kheeri',\n",
       " 'chicken_razala',\n",
       " 'chicken_tikka',\n",
       " 'chicken_tikka_masala',\n",
       " 'chikki',\n",
       " 'daal_baati_churma',\n",
       " 'daal_puri',\n",
       " 'dal_makhani',\n",
       " 'dal_tadka',\n",
       " 'dharwad_pedha',\n",
       " 'doodhpak',\n",
       " 'double_ka_meetha',\n",
       " 'dum_aloo',\n",
       " 'gajar_ka_halwa',\n",
       " 'gavvalu',\n",
       " 'ghevar',\n",
       " 'gulab_jamun',\n",
       " 'imarti',\n",
       " 'jalebi',\n",
       " 'kachori',\n",
       " 'kadai_paneer',\n",
       " 'kadhi_pakoda',\n",
       " 'kajjikaya',\n",
       " 'kakinada_khaja',\n",
       " 'kalakand',\n",
       " 'karela_bharta',\n",
       " 'kofta',\n",
       " 'kuzhi_paniyaram',\n",
       " 'lassi',\n",
       " 'ledikeni',\n",
       " 'litti_chokha',\n",
       " 'lyangcha',\n",
       " 'maach_jhol',\n",
       " 'makki_di_roti_sarson_da_saag',\n",
       " 'malapua',\n",
       " 'misi_roti',\n",
       " 'misti_doi',\n",
       " 'modak',\n",
       " 'mysore_pak',\n",
       " 'naan',\n",
       " 'navrattan_korma',\n",
       " 'palak_paneer',\n",
       " 'paneer_butter_masala',\n",
       " 'phirni',\n",
       " 'pithe',\n",
       " 'poha',\n",
       " 'poornalu',\n",
       " 'pootharekulu',\n",
       " 'qubani_ka_meetha',\n",
       " 'rabri',\n",
       " 'ras_malai',\n",
       " 'rasgulla',\n",
       " 'sandesh',\n",
       " 'shankarpali',\n",
       " 'sheer_korma',\n",
       " 'sheera',\n",
       " 'shrikhand',\n",
       " 'sohan_halwa',\n",
       " 'sohan_papdi',\n",
       " 'sutar_feni',\n",
       " 'unni_appam']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_images.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf284c4-1140-4ec9-8f53-1e84de620562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Model\n",
    "image_input = Input(shape=(128, 128, 3))\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "image_output = layers.Dense(128, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617f7a84-5e57-4d8d-a5c9-faa4724a301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Layer\n",
    "final_output = layers.Dense(len(label_encoder.classes_), activation='softmax')(image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64bfa04e-ceee-4ec5-919d-2f9497c61cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "model = models.Model(inputs=image_input, outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3848416b-9d51-4a98-9873-ecb7f305de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 77s 763ms/step - loss: 4.4321 - accuracy: 0.0178 - val_loss: 4.2591 - val_accuracy: 0.0350\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 4.0057 - accuracy: 0.0812 - val_loss: 3.9304 - val_accuracy: 0.0700\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 2.8102 - accuracy: 0.3303 - val_loss: 3.9348 - val_accuracy: 0.1775\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 33s 334ms/step - loss: 1.1954 - accuracy: 0.7222 - val_loss: 4.4024 - val_accuracy: 0.2412\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.4008 - accuracy: 0.9241 - val_loss: 5.3240 - val_accuracy: 0.2587\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.1687 - accuracy: 0.9753 - val_loss: 5.7840 - val_accuracy: 0.2650\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.0994 - accuracy: 0.9856 - val_loss: 5.6347 - val_accuracy: 0.2650\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 0.0627 - accuracy: 0.9897 - val_loss: 5.5410 - val_accuracy: 0.2663\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 0.0462 - accuracy: 0.9909 - val_loss: 6.0686 - val_accuracy: 0.2812\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.0547 - accuracy: 0.9891 - val_loss: 5.7747 - val_accuracy: 0.2750\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 0.0503 - accuracy: 0.9906 - val_loss: 5.6858 - val_accuracy: 0.2625\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 33s 332ms/step - loss: 0.0401 - accuracy: 0.9906 - val_loss: 5.5668 - val_accuracy: 0.2650\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 34s 341ms/step - loss: 0.0306 - accuracy: 0.9919 - val_loss: 5.9016 - val_accuracy: 0.2637\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 34s 336ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 5.7964 - val_accuracy: 0.2663\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 5.5303 - val_accuracy: 0.2637\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.0275 - accuracy: 0.9909 - val_loss: 5.7602 - val_accuracy: 0.2625\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0256 - accuracy: 0.9906 - val_loss: 5.7374 - val_accuracy: 0.2600\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 0.0275 - accuracy: 0.9906 - val_loss: 5.7200 - val_accuracy: 0.2675\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0235 - accuracy: 0.9916 - val_loss: 5.6377 - val_accuracy: 0.2637\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 5.7796 - val_accuracy: 0.2637\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 0.0226 - accuracy: 0.9909 - val_loss: 5.7488 - val_accuracy: 0.2625\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.0182 - accuracy: 0.9900 - val_loss: 5.9029 - val_accuracy: 0.2625\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.0190 - accuracy: 0.9906 - val_loss: 5.7670 - val_accuracy: 0.2663\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 0.0206 - accuracy: 0.9916 - val_loss: 5.7175 - val_accuracy: 0.2637\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0160 - accuracy: 0.9906 - val_loss: 6.2449 - val_accuracy: 0.2675\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.0190 - accuracy: 0.9900 - val_loss: 5.9903 - val_accuracy: 0.2650\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0220 - accuracy: 0.9887 - val_loss: 5.7784 - val_accuracy: 0.2637\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0171 - accuracy: 0.9922 - val_loss: 5.8802 - val_accuracy: 0.2675\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0733 - accuracy: 0.9825 - val_loss: 6.9465 - val_accuracy: 0.2375\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.4824 - accuracy: 0.8847 - val_loss: 6.7861 - val_accuracy: 0.2375\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.1931 - accuracy: 0.9444 - val_loss: 7.1714 - val_accuracy: 0.2525\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.0722 - accuracy: 0.9772 - val_loss: 7.2164 - val_accuracy: 0.2650\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0274 - accuracy: 0.9894 - val_loss: 7.4427 - val_accuracy: 0.2612\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0178 - accuracy: 0.9900 - val_loss: 7.5992 - val_accuracy: 0.2713\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0141 - accuracy: 0.9922 - val_loss: 7.4986 - val_accuracy: 0.2675\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.0117 - accuracy: 0.9922 - val_loss: 7.6121 - val_accuracy: 0.2675\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.0129 - accuracy: 0.9919 - val_loss: 7.5397 - val_accuracy: 0.2637\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0119 - accuracy: 0.9925 - val_loss: 7.6712 - val_accuracy: 0.2650\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0129 - accuracy: 0.9919 - val_loss: 7.5677 - val_accuracy: 0.2700\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.0115 - accuracy: 0.9925 - val_loss: 7.5928 - val_accuracy: 0.2675\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0116 - accuracy: 0.9909 - val_loss: 7.7194 - val_accuracy: 0.2637\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0116 - accuracy: 0.9919 - val_loss: 7.7309 - val_accuracy: 0.2725\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0114 - accuracy: 0.9928 - val_loss: 7.6903 - val_accuracy: 0.2688\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 0.0110 - accuracy: 0.9928 - val_loss: 7.6738 - val_accuracy: 0.2625\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0099 - accuracy: 0.9934 - val_loss: 7.7927 - val_accuracy: 0.2650\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0108 - accuracy: 0.9937 - val_loss: 7.8368 - val_accuracy: 0.2650\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0103 - accuracy: 0.9928 - val_loss: 7.9283 - val_accuracy: 0.2663\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0113 - accuracy: 0.9919 - val_loss: 7.8237 - val_accuracy: 0.2675\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0105 - accuracy: 0.9928 - val_loss: 7.9286 - val_accuracy: 0.2700\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0108 - accuracy: 0.9906 - val_loss: 7.8933 - val_accuracy: 0.2663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2adeda4bc10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "epochs = 50\n",
    "model.fit(\n",
    "    train_images,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71383770-06f6-4a88-b3a5-a62e8067cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model with additional metrics like Precision and Recall\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56f647c2-eb53-4150-aefa-8d547c01f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\metrics\\confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\metrics_utils.py\", line 674, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 80) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Accessing the history object for training metrics\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4te_fziq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\metrics\\confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\AKASH\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\metrics_utils.py\", line 674, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 80) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, epochs=10, validation_data=val_images)\n",
    "\n",
    "# Accessing the history object for training metrics\n",
    "print(f\"Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "print(f\"Training Precision: {history.history['precision'][-1]}\")\n",
    "print(f\"Validation Precision: {history.history['val_precision'][-1]}\")\n",
    "print(f\"Training Recall: {history.history['recall'][-1]}\")\n",
    "print(f\"Validation Recall: {history.history['val_recall'][-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e522ea1-0790-4c76-9cc4-417f9ea50da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.h5'\n",
      "model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model.h5')  # Save the Keras model in HDF5 format\n",
    "\n",
    "# Save the label encoder\n",
    "with open('model.pkl', 'wb') as encoder_file:\n",
    "    pickle.dump(label_encoder, encoder_file)\n",
    "\n",
    "print(\"model.h5'\")\n",
    "print(\"model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7519aff-b469-4571-845c-3558dc611b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Label Encoder loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('model.h5')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the saved label encoder\n",
    "with open('model.pkl', 'rb') as encoder_file:\n",
    "    loaded_encoder = pickle.load(encoder_file)\n",
    "print(\"Label Encoder loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97ff70c5-6af8-41a2-8cf7-a9e0e133cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "Predicted Food: butter_chicken\n",
      "Nutrients: {'Proteins (g)': 23.0, 'Carbs (g)': 16.5, ' Sugar (g)': 7.0, 'Fats (g)': 14.3}\n"
     ]
    }
   ],
   "source": [
    "def predict_food_and_nutrients_with_loaded_model(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(128, 128))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict using the loaded model\n",
    "    prediction = loaded_model.predict(img_array)\n",
    "    predicted_class = loaded_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "\n",
    "    # Retrieve the nutrients for the predicted food\n",
    "    nutrients = data[data['Food'] == predicted_class].iloc[:, 1:].to_dict('records')[0]\n",
    "    return predicted_class, nutrients\n",
    "\n",
    "# Example Usage\n",
    "example_image = '33e8639efd.jpg'\n",
    "predicted_food, predicted_nutrients = predict_food_and_nutrients_with_loaded_model(example_image)\n",
    "print(f'Predicted Food: {predicted_food}')\n",
    "print(f'Nutrients: {predicted_nutrients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fedab-e3a6-400d-b9c9-7555d8699720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
